{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d4adb11-cf0c-49b0-9bc0-67dcb26b618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import negative_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "965884cf-5aeb-4cec-bea5-2dfb8ce27bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db62308a-bb62-4e54-bcde-20ea56612aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "path = osp.join(osp.dirname(osp.realpath(os.__file__)), '..', 'data', 'Planetoid')\n",
    "dataset = Planetoid(path, name='Cora', transform=transform)\n",
    "# After applying the `RandomLinkSplit` transform, the data is transformed from\n",
    "# a data object to a list of tuples (train_data, val_data, test_data), with\n",
    "# each element representing the corresponding split.\n",
    "train_data, val_data, test_data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25e5ee8f-381e-4711-817b-7d81e4320f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "589533e4-fea5-4799-9183-d81d3e2ae044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6930, Val: 0.7362, Test: 0.7141\n",
      "Epoch: 002, Loss: 0.6821, Val: 0.7325, Test: 0.7103\n",
      "Epoch: 003, Loss: 0.7102, Val: 0.7257, Test: 0.7145\n",
      "Epoch: 004, Loss: 0.6769, Val: 0.7289, Test: 0.7350\n",
      "Epoch: 005, Loss: 0.6851, Val: 0.8040, Test: 0.8058\n",
      "Epoch: 006, Loss: 0.6886, Val: 0.8361, Test: 0.8152\n",
      "Epoch: 007, Loss: 0.6895, Val: 0.7904, Test: 0.7487\n",
      "Epoch: 008, Loss: 0.6892, Val: 0.7575, Test: 0.7087\n",
      "Epoch: 009, Loss: 0.6875, Val: 0.7489, Test: 0.6990\n",
      "Epoch: 010, Loss: 0.6844, Val: 0.7517, Test: 0.7043\n",
      "Epoch: 011, Loss: 0.6799, Val: 0.7583, Test: 0.7150\n",
      "Epoch: 012, Loss: 0.6765, Val: 0.7645, Test: 0.7327\n",
      "Epoch: 013, Loss: 0.6737, Val: 0.7588, Test: 0.7468\n",
      "Epoch: 014, Loss: 0.6671, Val: 0.7491, Test: 0.7561\n",
      "Epoch: 015, Loss: 0.6594, Val: 0.7387, Test: 0.7620\n",
      "Epoch: 016, Loss: 0.6510, Val: 0.7315, Test: 0.7641\n",
      "Epoch: 017, Loss: 0.6425, Val: 0.7314, Test: 0.7691\n",
      "Epoch: 018, Loss: 0.6319, Val: 0.7429, Test: 0.7866\n",
      "Epoch: 019, Loss: 0.6197, Val: 0.7754, Test: 0.8088\n",
      "Epoch: 020, Loss: 0.6088, Val: 0.7953, Test: 0.8179\n",
      "Epoch: 021, Loss: 0.5920, Val: 0.7940, Test: 0.8162\n",
      "Epoch: 022, Loss: 0.5811, Val: 0.7918, Test: 0.8146\n",
      "Epoch: 023, Loss: 0.5645, Val: 0.7927, Test: 0.8142\n",
      "Epoch: 024, Loss: 0.5650, Val: 0.7937, Test: 0.8148\n",
      "Epoch: 025, Loss: 0.5658, Val: 0.7974, Test: 0.8186\n",
      "Epoch: 026, Loss: 0.5558, Val: 0.8058, Test: 0.8243\n",
      "Epoch: 027, Loss: 0.5464, Val: 0.8198, Test: 0.8279\n",
      "Epoch: 028, Loss: 0.5368, Val: 0.8319, Test: 0.8373\n",
      "Epoch: 029, Loss: 0.5317, Val: 0.8441, Test: 0.8444\n",
      "Epoch: 030, Loss: 0.5167, Val: 0.8534, Test: 0.8495\n",
      "Epoch: 031, Loss: 0.5163, Val: 0.8620, Test: 0.8524\n",
      "Epoch: 032, Loss: 0.5079, Val: 0.8671, Test: 0.8557\n",
      "Epoch: 033, Loss: 0.5032, Val: 0.8703, Test: 0.8604\n",
      "Epoch: 034, Loss: 0.4970, Val: 0.8762, Test: 0.8627\n",
      "Epoch: 035, Loss: 0.4922, Val: 0.8814, Test: 0.8616\n",
      "Epoch: 036, Loss: 0.4970, Val: 0.8805, Test: 0.8622\n",
      "Epoch: 037, Loss: 0.4983, Val: 0.8877, Test: 0.8683\n",
      "Epoch: 038, Loss: 0.4936, Val: 0.8927, Test: 0.8731\n",
      "Epoch: 039, Loss: 0.4885, Val: 0.8993, Test: 0.8768\n",
      "Epoch: 040, Loss: 0.4816, Val: 0.9044, Test: 0.8776\n",
      "Epoch: 041, Loss: 0.4712, Val: 0.9071, Test: 0.8801\n",
      "Epoch: 042, Loss: 0.4691, Val: 0.9080, Test: 0.8853\n",
      "Epoch: 043, Loss: 0.4688, Val: 0.9071, Test: 0.8870\n",
      "Epoch: 044, Loss: 0.4842, Val: 0.9079, Test: 0.8845\n",
      "Epoch: 045, Loss: 0.4699, Val: 0.9095, Test: 0.8853\n",
      "Epoch: 046, Loss: 0.4728, Val: 0.9089, Test: 0.8884\n",
      "Epoch: 047, Loss: 0.4707, Val: 0.9069, Test: 0.8914\n",
      "Epoch: 048, Loss: 0.4760, Val: 0.9072, Test: 0.8922\n",
      "Epoch: 049, Loss: 0.4633, Val: 0.9099, Test: 0.8939\n",
      "Epoch: 050, Loss: 0.4591, Val: 0.9111, Test: 0.8958\n",
      "Epoch: 051, Loss: 0.4644, Val: 0.9101, Test: 0.8971\n",
      "Epoch: 052, Loss: 0.4618, Val: 0.9117, Test: 0.8978\n",
      "Epoch: 053, Loss: 0.4527, Val: 0.9150, Test: 0.8995\n",
      "Epoch: 054, Loss: 0.4594, Val: 0.9189, Test: 0.9021\n",
      "Epoch: 055, Loss: 0.4559, Val: 0.9191, Test: 0.9053\n",
      "Epoch: 056, Loss: 0.4538, Val: 0.9189, Test: 0.9058\n",
      "Epoch: 057, Loss: 0.4508, Val: 0.9195, Test: 0.9051\n",
      "Epoch: 058, Loss: 0.4575, Val: 0.9208, Test: 0.9066\n",
      "Epoch: 059, Loss: 0.4495, Val: 0.9221, Test: 0.9110\n",
      "Epoch: 060, Loss: 0.4469, Val: 0.9221, Test: 0.9140\n",
      "Epoch: 061, Loss: 0.4546, Val: 0.9215, Test: 0.9145\n",
      "Epoch: 062, Loss: 0.4522, Val: 0.9220, Test: 0.9129\n",
      "Epoch: 063, Loss: 0.4547, Val: 0.9227, Test: 0.9121\n",
      "Epoch: 064, Loss: 0.4472, Val: 0.9227, Test: 0.9147\n",
      "Epoch: 065, Loss: 0.4414, Val: 0.9202, Test: 0.9175\n",
      "Epoch: 066, Loss: 0.4465, Val: 0.9201, Test: 0.9176\n",
      "Epoch: 067, Loss: 0.4509, Val: 0.9210, Test: 0.9150\n",
      "Epoch: 068, Loss: 0.4485, Val: 0.9242, Test: 0.9152\n",
      "Epoch: 069, Loss: 0.4477, Val: 0.9239, Test: 0.9168\n",
      "Epoch: 070, Loss: 0.4418, Val: 0.9215, Test: 0.9179\n",
      "Epoch: 071, Loss: 0.4416, Val: 0.9197, Test: 0.9175\n",
      "Epoch: 072, Loss: 0.4441, Val: 0.9215, Test: 0.9171\n",
      "Epoch: 073, Loss: 0.4425, Val: 0.9231, Test: 0.9150\n",
      "Epoch: 074, Loss: 0.4464, Val: 0.9230, Test: 0.9141\n",
      "Epoch: 075, Loss: 0.4397, Val: 0.9231, Test: 0.9166\n",
      "Epoch: 076, Loss: 0.4396, Val: 0.9219, Test: 0.9192\n",
      "Epoch: 077, Loss: 0.4440, Val: 0.9226, Test: 0.9206\n",
      "Epoch: 078, Loss: 0.4384, Val: 0.9238, Test: 0.9203\n",
      "Epoch: 079, Loss: 0.4443, Val: 0.9239, Test: 0.9185\n",
      "Epoch: 080, Loss: 0.4419, Val: 0.9230, Test: 0.9195\n",
      "Epoch: 081, Loss: 0.4360, Val: 0.9222, Test: 0.9201\n",
      "Epoch: 082, Loss: 0.4429, Val: 0.9217, Test: 0.9202\n",
      "Epoch: 083, Loss: 0.4386, Val: 0.9238, Test: 0.9213\n",
      "Epoch: 084, Loss: 0.4422, Val: 0.9238, Test: 0.9216\n",
      "Epoch: 085, Loss: 0.4413, Val: 0.9237, Test: 0.9211\n",
      "Epoch: 086, Loss: 0.4341, Val: 0.9237, Test: 0.9218\n",
      "Epoch: 087, Loss: 0.4378, Val: 0.9246, Test: 0.9224\n",
      "Epoch: 088, Loss: 0.4390, Val: 0.9254, Test: 0.9235\n",
      "Epoch: 089, Loss: 0.4362, Val: 0.9265, Test: 0.9246\n",
      "Epoch: 090, Loss: 0.4335, Val: 0.9281, Test: 0.9250\n",
      "Epoch: 091, Loss: 0.4325, Val: 0.9283, Test: 0.9250\n",
      "Epoch: 092, Loss: 0.4379, Val: 0.9285, Test: 0.9244\n",
      "Epoch: 093, Loss: 0.4348, Val: 0.9303, Test: 0.9247\n",
      "Epoch: 094, Loss: 0.4373, Val: 0.9314, Test: 0.9247\n",
      "Epoch: 095, Loss: 0.4243, Val: 0.9323, Test: 0.9249\n",
      "Epoch: 096, Loss: 0.4346, Val: 0.9311, Test: 0.9250\n",
      "Epoch: 097, Loss: 0.4316, Val: 0.9322, Test: 0.9251\n",
      "Epoch: 098, Loss: 0.4318, Val: 0.9343, Test: 0.9246\n",
      "Epoch: 099, Loss: 0.4350, Val: 0.9350, Test: 0.9236\n",
      "Epoch: 100, Loss: 0.4265, Val: 0.9350, Test: 0.9244\n",
      "Final Test: 0.9244\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = Net(dataset.num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2080cd2-6ab8-4053-806d-011a17fa9188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
